{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(filename):\n",
    "\tvectors = {}\n",
    "\twith open(filename, 'r') as f:\n",
    "\t\treader = csv.reader(f, delimiter = ' ')\n",
    "\t\tfor row in reader:\n",
    "\t\t\tword = re.sub('[^a-z]+', '', row[0].strip().lower())\n",
    "\t\t\tif len(word) < 2: continue\n",
    "\t\t\tvectors[word] = [float(x) for x in row[1:] if len(x) >0]\n",
    "\treturn vectors\n",
    "\n",
    "def find_vector_norms(vectors):\n",
    "\tnorms = [np.linalg.norm(vectors[word]) for word in vectors]\n",
    "\treturn np.mean(norms), np.var(norms), np.median(norms)\n",
    "\n",
    "\n",
    "def print_sizes(folder = '../vectors/normalized_clean/'):\n",
    "\tfilenames_sgns = [folder + 'vectors_sgns{}.txt'.format(x) for x in range(1910, 2000, 10)]\n",
    "\tfilenames_svd = [folder + 'vectors_svd{}.txt'.format(x) for x in range(1910, 2000, 10)]\n",
    "\tfilenames_nyt = [folder + 'vectors{}-{}.txt'.format(x, x+5) for x in range(1987, 2000, 1)]\n",
    "\tfilenames_coha = [folder + 'vectorscoha{}-{}.txt'.format(x, x+20) for x in range(1910, 2000, 10)]\n",
    "\n",
    "\tfilenames_combined = [filenames_nyt, filenames_sgns, filenames_svd, [folder + 'vectorswikipedia.txt'], [folder + 'vectorsGoogleNews_exactclean.txt']]\n",
    "\n",
    "\tfor names in filenames_combined:\n",
    "\t\tfor name in names:\n",
    "\t\t\tprint(name, find_vector_norms(load_vectors(name)))\n",
    "\n",
    "def normalize(filename, filename_output):\n",
    "\tvectors = {}\n",
    "\tcountnorm0 = 0\n",
    "\tcountnormal = 0\n",
    "\twith open(filename_output, 'w') as fo:\n",
    "\t\twriter = csv.writer(fo, delimiter = ' ')\n",
    "\t\twith open(filename, 'r') as f:\n",
    "\t\t\treader = csv.reader(f, delimiter = ' ')\n",
    "\t\t\tfor row in reader:\n",
    "\t\t\t\trowout = row\n",
    "\t\t\t\tword = re.sub('[^a-z]+', '', row[0].strip().lower())\n",
    "\t\t\t\trowout[0] = word\n",
    "\t\t\t\tif len(word) < 2: continue\n",
    "\t\t\t\t# print(word)\n",
    "\t\t\t\tnorm = np.linalg.norm([float(x) for x in row[1:] if len(x) >0])\n",
    "\t\t\t\tif norm < 1e-2:\n",
    "\t\t\t\t\tcountnorm0+=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcountnormal+=1\n",
    "\t\t\t\t\tfor en in range(1, len(rowout)):\n",
    "\t\t\t\t\t\tif len(rowout[en])>0:\n",
    "\t\t\t\t\t\t\trowout[en] = float(rowout[en])/norm\n",
    "\t\t\t\t\twriter.writerow(rowout)\n",
    "\t\tfo.flush()\n",
    "\tprint (countnorm0, countnormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 397\n",
      "0 10619\n",
      "0 12083\n",
      "0 12532\n",
      "0 12750\n",
      "0 11100\n",
      "0 11114\n",
      "0 12230\n",
      "0 11428\n",
      "0 11547\n"
     ]
    }
   ],
   "source": [
    "filenames_movie_vectors = ['vectors-{}.txt'.format(x) for x in range(1920, 2020, 10)]\n",
    "\n",
    "for file in filenames_movie_vectors:\n",
    "    normalize('data/final_project/vectors/base/' + file, 'data/final_project/vectors/normalized_clean/' + file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bffae61b4a647f5c3a2b9689cc2bbc523f2c1227093a073b0d809d31c71c3583"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
